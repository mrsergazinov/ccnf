{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import distributions\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "# from latentcor import gen_data, get_tps, latentcor\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 500)\n"
     ]
    }
   ],
   "source": [
    "# load\n",
    "data = pd.read_csv('./sim-data-mult/trans_series.csv', index_col=0).to_numpy(dtype=\"float32\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train_data = {'x': data[:9000, :-50], 'y': data[:9000, -50:]}\n",
    "test_data = {'x': data[9000:, :-50], 'y': data[9000:, -50:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # init\n",
    "        self.data = data\n",
    "    def __getitem__(self, index):\n",
    "        return self.data['x'][index, :], self.data['y'][index, :]\n",
    "    def __len__(self):\n",
    "        return self.data['x'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale\n",
    "mu = np.mean(np.concatenate([train_data['x'], train_data['y']], 1))\n",
    "std = np.std(np.concatenate([train_data['x'], train_data['y']], 1))\n",
    "\n",
    "train_data['x'] = (train_data['x'] - mu) / std\n",
    "train_data['y'] = (train_data['y'] - mu) / std\n",
    "test_data['x'] = (test_data['x'] - mu) / std\n",
    "test_data['y'] = (test_data['y'] - mu) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate covariance\n",
    "phi = 0.9\n",
    "sigmasq = 0.0001\n",
    "n = 500\n",
    "\n",
    "true_cov = np.zeros((n, n))\n",
    "row = np.cumprod([1] + [phi]*499)\n",
    "for i in range(n):\n",
    "    true_cov[i, :(i+1)] = np.flip(row[:(i+1)])\n",
    "    true_cov[i, (i+1):] = row[1:(n-i)]\n",
    "true_cov = true_cov.astype(np.float32)\n",
    "true_cov = true_cov * sigmasq / (1 - phi**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader for training data\n",
    "train_dataset = Data(train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_loader): break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 450])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, nets1, nett1, mask1, \n",
    "                 nets2, nett2, mask2, prior1, prior2, pred_mat, pred_len):\n",
    "        super(RealNVP, self).__init__()\n",
    "        \n",
    "        self.pred_len = pred_len\n",
    "        self.pred_mat = nn.Parameter(data=pred_mat, requires_grad=False)\n",
    "        \n",
    "        self.prior1 = prior1\n",
    "        self.prior2 = prior2\n",
    "        \n",
    "        self.mask1 = nn.Parameter(mask1, requires_grad=False)\n",
    "        self.t1 = torch.nn.ModuleList([nett1() for _ in range(len(mask1))])\n",
    "        self.s1 = torch.nn.ModuleList([nets1() for _ in range(len(mask1))])\n",
    "        \n",
    "        self.mask2 = nn.Parameter(mask2, requires_grad=False)\n",
    "        self.t2 = torch.nn.ModuleList([nett2() for _ in range(len(mask2))])\n",
    "        self.s2 = torch.nn.ModuleList([nets2() for _ in range(len(mask2))])\n",
    "        \n",
    "    def g(self, x1, x2):\n",
    "        # send to latent and predict (with Gaussian)\n",
    "        z1, z2, _ = self.f(x1, x2)\n",
    "        z2 = torch.matmul(self.pred_mat, z1.permute(1, 0)).permute(1, 0)\n",
    "        x1_inv, x2 = z1, z2\n",
    "        \n",
    "        for i in range(len(self.t1)):\n",
    "            x1_ = x1_inv * self.mask1[i]\n",
    "            s1 = self.s1[i](x1_)\n",
    "            t1 = self.t1[i](x1_)\n",
    "            x1_inv = x1_ + (1 - self.mask1[i]) * (x1_inv * torch.exp(s1) + t1)\n",
    "            \n",
    "        for i in range(len(self.t1)):\n",
    "            x2_ = x2 * self.mask2[i]\n",
    "            x2cond_ = torch.cat([x2_, x1], 1)\n",
    "            s2 = self.s2[i](x2cond_)\n",
    "            t2 = self.t2[i](x2cond_)\n",
    "            x2 = x2_ + (1 - self.mask2[i]) * (x2 * torch.exp(s2) + t2)\n",
    "            \n",
    "        return x1_inv, x2\n",
    "\n",
    "    def f(self, x1, x2):\n",
    "        log_det_J1, z1 = x1.new_zeros(x1.shape[0]), x1\n",
    "        for i in reversed(range(len(self.t1))):\n",
    "            z1_ = self.mask1[i] * z1\n",
    "            s1 = self.s1[i](z1_)\n",
    "            t1 = self.t1[i](z1_)\n",
    "            z1 = (1 - self.mask1[i]) * ((z1 - t1) * torch.exp(-s1)) + z1_\n",
    "            log_det_J1 -= s1.sum(dim=1)\n",
    "        \n",
    "        log_det_J2, z2 = x2.new_zeros(x2.shape[0]), x2\n",
    "        for i in reversed(range(len(self.t1))):\n",
    "            z2_ = self.mask2[i] * z2\n",
    "            z2cond_ = torch.cat([z2_, x1], 1)\n",
    "            s2 = self.s2[i](z2cond_)\n",
    "            t2 = self.t2[i](z2cond_)\n",
    "            z2 = (1 - self.mask2[i]) * ((z2 - t2) * torch.exp(-s2)) + z2_\n",
    "            log_det_J2 -= s2.sum(dim=1)\n",
    "        \n",
    "        return z1, z2, log_det_J1+log_det_J2\n",
    "    \n",
    "    def log_prob(self,x1,x2):\n",
    "        z1, z2, logp = self.f(x1, x2)\n",
    "        return self.prior1.log_prob(z1) + self.prior2.log_prob(z2) + logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/m/mrsergazinov/.conda/envs/copula/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RealNVP(\n",
       "  (t1): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=450, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=450, bias=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=450, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=450, bias=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=450, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=450, bias=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=450, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=450, bias=True)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Linear(in_features=450, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=450, bias=True)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Linear(in_features=450, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=450, bias=True)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Linear(in_features=450, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=450, bias=True)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Linear(in_features=450, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=450, bias=True)\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Linear(in_features=450, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=450, bias=True)\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Linear(in_features=450, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=450, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (s1): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=450, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=450, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=450, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=450, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=450, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=450, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=450, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=450, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Linear(in_features=450, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=450, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Linear(in_features=450, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=450, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Linear(in_features=450, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=450, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Linear(in_features=450, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=450, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Linear(in_features=450, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=450, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Linear(in_features=450, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=450, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (t2): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=500, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=50, bias=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=500, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=50, bias=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=500, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=50, bias=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=500, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=50, bias=True)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Linear(in_features=500, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=50, bias=True)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Linear(in_features=500, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=50, bias=True)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Linear(in_features=500, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=50, bias=True)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Linear(in_features=500, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=50, bias=True)\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Linear(in_features=500, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=50, bias=True)\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Linear(in_features=500, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=50, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (s2): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=500, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=50, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=500, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=50, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=500, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=50, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=500, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=50, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Linear(in_features=500, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=50, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Linear(in_features=500, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=50, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Linear(in_features=500, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=50, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Linear(in_features=500, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=50, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Linear(in_features=500, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=50, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Linear(in_features=500, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=50, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 500\n",
    "pred_len = 50\n",
    "\n",
    "nets1 = lambda: nn.Sequential(nn.Linear(450, 1024), \n",
    "                             nn.LeakyReLU(), \n",
    "                             nn.Linear(1024, 1024), \n",
    "                             nn.LeakyReLU(), \n",
    "                             nn.Linear(1024, 450), \n",
    "                             nn.Tanh())\n",
    "nett1 = lambda: nn.Sequential(nn.Linear(450, 1024), \n",
    "                             nn.LeakyReLU(), \n",
    "                             nn.Linear(1024, 1024), \n",
    "                             nn.LeakyReLU(), \n",
    "                             nn.Linear(1024, 450))\n",
    "\n",
    "nets2 = lambda: nn.Sequential(nn.Linear(500, 1024), \n",
    "                             nn.LeakyReLU(), \n",
    "                             nn.Linear(1024, 1024), \n",
    "                             nn.LeakyReLU(), \n",
    "                             nn.Linear(1024, 50), \n",
    "                             nn.Tanh())\n",
    "nett2 = lambda: nn.Sequential(nn.Linear(500, 1024), \n",
    "                             nn.LeakyReLU(), \n",
    "                             nn.Linear(1024, 1024), \n",
    "                             nn.LeakyReLU(), \n",
    "                             nn.Linear(1024, 50))\n",
    "\n",
    "mask1_x = [0]*((n - pred_len) // 2) + [1]*((n - pred_len) // 2)\n",
    "mask2_x = [1]*((n - pred_len) // 2) + [0]*((n - pred_len) // 2)\n",
    "mask1_y = [0]*(pred_len // 2) + [1]*(pred_len // 2)\n",
    "mask2_y = [1]*(pred_len // 2) + [0]*(pred_len // 2)\n",
    "masks_x = torch.from_numpy(np.array([mask1_x, mask2_x] * 5).astype(np.float32))\n",
    "masks_y = torch.from_numpy(np.array([mask1_y, mask2_y] * 5).astype(np.float32))\n",
    "\n",
    "true_cov = torch.tensor(true_cov)\n",
    "pred_mat = torch.matmul(true_cov[-pred_len:, :-pred_len], true_cov[:-pred_len, :-pred_len].inverse())\n",
    "cov_cond = true_cov[-pred_len:, -pred_len:] - torch.matmul(pred_mat, true_cov[:-pred_len, -pred_len:])\n",
    "prior1 = distributions.MultivariateNormal(torch.zeros(n-pred_len).cuda(), \n",
    "                                          true_cov[:(n-pred_len), :(n-pred_len)].cuda())\n",
    "prior2 = distributions.MultivariateNormal(torch.zeros(pred_len).cuda(), cov_cond.cuda())\n",
    "\n",
    "flow = RealNVP(nets1, nett1, masks_x, nets2, nett2, masks_y, prior1, prior2, pred_mat, pred_len)\n",
    "flow.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_57890/2803771015.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0miter_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_57890/2543753730.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlogp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/copula/lib/python3.7/site-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_batch_mahalanobis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mhalf_log_det\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "TRAIN_STEPS = len(train_loader)\n",
    "\n",
    "optimizer = torch.optim.Adam([p for p in flow.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "for epoch in range(EPOCHS):\n",
    "    iter_count = 0\n",
    "    train_loss = []\n",
    "    epoch_time = time.time()\n",
    "    curr_time = time.time()\n",
    "    \n",
    "    for i, (x1, x2) in enumerate(train_loader):\n",
    "        iter_count += 1\n",
    "        x1, x2 = x1.cuda(), x2.cuda()\n",
    "        loss = -flow.log_prob(x1, x2).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        if iter_count % 10 == 0:\n",
    "            print('\\titers: {0}, epoch: {1} | loss: {2:.3f}'.format(iter_count, epoch + 1, loss.item()))\n",
    "            speed = (time.time() - curr_time) / iter_count\n",
    "            left_time = speed * ((EPOCHS - epoch) * TRAIN_STEPS - i)\n",
    "            print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "    \n",
    "    # compute average train loss\n",
    "    train_loss = np.average(train_loss)\n",
    "    print(\"epoch: {}, epoch time: {}, loss: {}\".format(epoch+1, time.time() - curr_time, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(flow.state_dict(), './model.pt')\n",
    "torch.save(optimizer.state_dict(), './optim.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.load_state_dict(torch.load('./model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad test data\n",
    "test_data_padded = {'x': np.copy(test_data['x']), 'y': np.copy(test_data['y'])}\n",
    "test_data_padded['y'] = torch.zeros_like(torch.tensor(test_data['y']))\n",
    "test_data_padded['x'] = torch.tensor(test_data['x'])\n",
    "\n",
    "# predict\n",
    "x, y = flow.g(test_data_padded['x'], test_data_padded['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y[0, :].detach().cpu().numpy())\n",
    "plt.plot(test_data['y'][0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_data['y'][0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # predict last 24\n",
    "\n",
    "# test_data\n",
    "\n",
    "\n",
    "# for index in range(data_test.shape[0]):\n",
    "#     # pad with mean\n",
    "#     sample = np.concatenate((data_test[index, :-24], [np.mean(data_test[index, :-24])]*24))[np.newaxis, :]\n",
    "#     # iterate prediction step until convergence\n",
    "#     print('Index: {0}'.format(index))\n",
    "#     for i in range(5):\n",
    "#         z, logp = flow.f(torch.from_numpy(sample))\n",
    "#         print('\\tlog prob: {0:.4f}'.format(logp.item()))\n",
    "#         z = z.detach().numpy()[0]\n",
    "#         zhat = cov[-24:, :-24] @ np.linalg.inv(cov[:-24, :-24]) @ z[:-24]\n",
    "#         z = np.concatenate((z[:-24], zhat))\n",
    "#         sample = flow.g(torch.from_numpy(z[np.newaxis, :])).detach().numpy()\n",
    "#     # compute confidence band\n",
    "#     z_upper = np.concatenate((z[:-24], zhat + 2*np.sqrt(np.diag(pred_cov))))\n",
    "#     z_lower = np.concatenate((z[:-24], zhat - 2*np.sqrt(np.diag(pred_cov))))\n",
    "#     sample_upper = flow.g(torch.from_numpy(z_lower[np.newaxis, :])).detach().numpy()\n",
    "#     sample_lower = flow.g(torch.from_numpy(z_upper[np.newaxis, :])).detach().numpy()\n",
    "#     # save results\n",
    "#     pred[index, :] = sample[0, :]; pred_lower[index, :] = sample_lower[0, :]; pred_upper[index, :] = sample_upper[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true = scaler.inverse_transform(data_test)[:, -24:]\n",
    "# pred = scaler.inverse_transform(pred)[:, -24:]\n",
    "# pred_lower = scaler.inverse_transform(pred_lower)[:, -24:]\n",
    "# pred_upper = scaler.inverse_transform(pred_upper)[:, -24:]\n",
    "# print(np.mean((true - pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 1\n",
    "# plt.plot(true[index, :], color='red')\n",
    "# plt.plot(pred[index, :], color='blue')\n",
    "# plt.plot(pred_lower[index, :], color='blue')\n",
    "# plt.plot(pred_upper[index, :], color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(np.mean((true-pred)**2, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
